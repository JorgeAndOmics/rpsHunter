"""
    Parses RPSBPROC output text files into structured CSV and Parquet tables.

    Iterates over all `.txt` output files from rpsbproc, extracts domain annotations,
    and writes results to both CSV and Parquet formats for downstream analysis.
"""

# -----------------------------------------------------------------------------
# DEPENDENCIES
# -----------------------------------------------------------------------------

import os
import csv
import logging
from typing import List, Dict, Any

from tqdm import tqdm
import pandas as pd

import defaults
import colored_logging


# -----------------------------------------------------------------------------
# PARSE RPSBPROC OUTPUT FUNCTION
# -----------------------------------------------------------------------------

def parse_rpsbproc_output(input_dir: str, output_dir: str) -> None:
    """
    Parses all RPSBPROC output files in the input directory and writes tabular results to disk.

        Parameters
        ----------
            :param input_dir: Path to directory containing `.txt` files generated by `rpsbproc`.
            :param output_dir: Path to directory where CSV and Parquet output files will be saved.

        Returns
        -------
            :returns: None. Writes results to `domains.csv` and `domains.parquet`.

        Raises
        ------
            :raises FileNotFoundError: If input directory does not exist.
            :raises IOError: If output files cannot be written.
    """
    if not os.path.exists(input_dir):
        raise FileNotFoundError(f'Input directory not found: {input_dir}')

    domain_hits: List[Dict[str, Any]] = []
    fieldnames: List[str] = [
        'Species', 'Session_ordinal', 'Program', 'Version', 'Database', 'Score_matrix', 'Evalue_threshold',
        'Query_ID', 'Seq_type', 'Seq_length', 'Definition', 'Chromosome', 'Start', 'End',
        'Hit_type', 'PSSM_ID', 'From', 'To', 'Evalue', 'Bitscore',
        'Accession', 'Domain', 'Incomplete', 'Superfamily_PSSM_ID'
    ]

    files: List[str] = [
        f for f in os.listdir(input_dir)
        if os.path.isfile(os.path.join(input_dir, f))
    ]

    total_files: int = len(files)
    file_bar = tqdm(total=total_files, desc='Processing files...')

    for filename in files:
        input_file = os.path.join(input_dir, filename)
        if not os.path.isfile(input_file):
            file_bar.update(1)
            continue

        logging.info(f'Processing file: {input_file}...')
        data_started = False
        current_session: Dict[str, str] = {}
        current_query: Dict[str, str] = {}
        file_name_without_ext = os.path.splitext(filename)[0]

        with open(input_file, 'r') as file:
            lines = file.readlines()

        index: int = 0
        total_lines: int = len(lines)

        while index < total_lines:
            line = lines[index].strip()

            if not data_started:
                if line == 'DATA':
                    data_started = True
                index += 1
                continue

            index += 1

            if line.startswith('SESSION'):
                parts = line.split('\t')
                current_session = {
                    'Session_ordinal': parts[1],
                    'Program': parts[2],
                    'Version': parts[3],
                    'Database': parts[4],
                    'Score_matrix': parts[5],
                    'Evalue_threshold': parts[6]
                }

            elif line.startswith('QUERY'):
                parts = line.split('\t')
                current_query = {
                    'Query_ID': parts[1],
                    'Seq_type': parts[2],
                    'Seq_length': parts[3],
                    'Definition': parts[4]
                }
                locations_in_query = {
                    'Chromosome': parts[4].split(':')[0] if ':' in parts[4] else '',
                    'Start': parts[4].split(':')[1].split('-')[0] if '-' in parts[4] else '',
                    'End': parts[4].split(':')[1].split('-')[1] if '-' in parts[4] else ''
                }
                if index < total_lines and lines[index].strip() == 'DOMAINS':
                    index += 1
                    while index < total_lines and not lines[index].strip().startswith('ENDDOMAINS'):
                        domain_line = lines[index].strip()
                        domain_parts = domain_line.split('\t')
                        domain_hit = {
                            'Species': file_name_without_ext,
                            'Session_ordinal': domain_parts[0],
                            'Query_ID': domain_parts[1],
                            'Hit_type': domain_parts[2],
                            'PSSM_ID': domain_parts[3],
                            'From': domain_parts[4],
                            'To': domain_parts[5],
                            'Evalue': domain_parts[6],
                            'Bitscore': domain_parts[7],
                            'Accession': domain_parts[8],
                            'Domain': domain_parts[9],
                            'Incomplete': domain_parts[10],
                            'Superfamily_PSSM_ID': domain_parts[11],
                            'Seq_type': current_query['Seq_type'],
                            'Seq_length': current_query['Seq_length'],
                            'Definition': current_query['Definition'],
                            'Chromosome': locations_in_query['Chromosome'],
                            'Start': locations_in_query['Start'],
                            'End': locations_in_query['End'],
                            'Program': current_session['Program'],
                            'Version': current_session['Version'],
                            'Database': current_session['Database'],
                            'Score_matrix': current_session['Score_matrix'],
                            'Evalue_threshold': current_session['Evalue_threshold']
                        }
                        domain_hits.append(domain_hit)

                        index += 1

                    index += 1  # Skip ENDDOMAINS

                else:
                    no_domain_hit = {
                        'Species': file_name_without_ext,
                        'Session_ordinal': current_session.get('Session_ordinal', ''),
                        'Program': current_session.get('Program', ''),
                        'Version': current_session.get('Version', ''),
                        'Database': current_session.get('Database', ''),
                        'Score_matrix': current_session.get('Score_matrix', ''),
                        'Evalue_threshold': current_session.get('Evalue_threshold', ''),
                        'Query_ID': current_query.get('Query_ID', ''),
                        'Seq_type': current_query.get('Seq_type', ''),
                        'Seq_length': current_query.get('Seq_length', ''),
                        'Definition': current_query.get('Definition', ''),
                        'Chromosome': locations_in_query.get('Chromosome', ''),
                        'Start': locations_in_query.get('Start', ''),
                        'End': locations_in_query.get('End', ''),
                        'Hit_type': '',
                        'PSSM_ID': '',
                        'From': '',
                        'To': '',
                        'Evalue': '',
                        'Bitscore': '',
                        'Accession': '',
                        'Domain': '',
                        'Incomplete': '',
                        'Superfamily_PSSM_ID': ''
                    }
                    domain_hits.append(no_domain_hit)

                if index < total_lines and lines[index].strip() == 'ENDQUERY':

                    index += 1

        file_bar.update(1)

    file_bar.close()

    # -------------------------------------------------------------------------
    # Write CSV Output
    # -------------------------------------------------------------------------
    csv_output_file = os.path.join(output_dir, 'domains.csv')
    logging.info(f'Writing CSV to {csv_output_file}')

    with open(csv_output_file, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\t')
        writer.writeheader()
        hits_bar = tqdm(total=len(domain_hits), desc='Writing CSV', leave=False)
        for hit in domain_hits:
            writer.writerow(hit)
            hits_bar.update(1)
        hits_bar.close()

    logging.info(f'CSV output written to {csv_output_file}')

    # -------------------------------------------------------------------------
    # Write Parquet Output
    # -------------------------------------------------------------------------
    logging.info('Writing Parquet file...')
    df = pd.DataFrame(domain_hits)
    parquet_output_file = os.path.join(output_dir, 'domains.parquet')
    df.to_parquet(parquet_output_file, index=False)
    logging.info(f'Parquet output written to {parquet_output_file}')


# -----------------------------------------------------------------------------
# SCRIPT ENTRY POINT
# -----------------------------------------------------------------------------

if __name__ == '__main__':
    colored_logging.colored_logging(log_file_name='rpsbproc_parser.txt')
    parse_rpsbproc_output(
        input_dir=defaults.PATH_DICT['RPSBPROC_OUTPUT_DIR'],
        output_dir=defaults.PATH_DICT['TABLE_OUTPUT_DIR']
    )
